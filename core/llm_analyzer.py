"""
LLM ê¸°ë°˜ ì½”ë“œ ë³´ì•ˆ ë¶„ì„ ëª¨ë“ˆ
"""
from openai import OpenAI
import json
import os
from typing import Dict, List, Optional

# RAGëŠ” ì„ íƒì  ì„í¬íŠ¸
try:
    from rag.simple_rag import SimpleRAG
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
    print("Warning: SimpleRAG not available")

# í”„ë¡¬í”„íŠ¸ëŠ” ì„ íƒì  ì„í¬íŠ¸
try:
    from prompts.security_prompts import (
        SYSTEM_PROMPT,
        get_analysis_prompt,
        get_validation_prompt,
        get_rag_integration_prompt
    )
    PROMPTS_AVAILABLE = True
except ImportError:
    PROMPTS_AVAILABLE = False
    # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    SYSTEM_PROMPT = "You are a Python security expert. Respond with JSON only."
    def get_analysis_prompt(code): 
        return f"Analyze this code for security vulnerabilities:\n{code}"


class LLMSecurityAnalyzer:
    """GPT ê¸°ë°˜ ë³´ì•ˆ ë¶„ì„ê¸°"""
    
    def __init__(self):
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.client = OpenAI(api_key=api_key)
        self.model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
        
        # RAG ì‹œìŠ¤í…œ (ì„ íƒì )
        self.rag = None
        self.rag_available = False
        
        if RAG_AVAILABLE:
            try:
                self.rag = SimpleRAG()
                self.rag_available = True
                print("âœ… RAG ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def analyze_code_security(self, code: str, context: Dict = None) -> Dict:
        """
        ì½”ë“œ ë³´ì•ˆ ë¶„ì„ ë©”ì¸ í•¨ìˆ˜
        """
        print("ğŸ” AI ë³´ì•ˆ ë¶„ì„ ì‹œì‘...")
        
        # GPT ë¶„ì„
        result = self._gpt_analyze(code)
        
        if not result or not result.get('vulnerabilities'):
            return {
                "success": True,
                "analysis": {
                    "code_vulnerabilities": [],
                    "security_score": 100,
                    "summary": "ë³´ì•ˆ ì·¨ì•½ì ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    "immediate_actions": [],
                    "best_practices": ["í˜„ì¬ ì½”ë“œëŠ” ê¸°ë³¸ì ì¸ ë³´ì•ˆ ê¸°ì¤€ì„ ì¶©ì¡±í•©ë‹ˆë‹¤."]
                }
            }
        
        vulnerabilities = result.get('vulnerabilities', [])
        
        # RAGë¡œ ì„¤ëª… ë³´ê°• (ì„ íƒì )
        if self.rag_available:
            self._enhance_with_rag(vulnerabilities)
        
        # ë³´ì•ˆ ì ìˆ˜ ë° ê¶Œì¥ì‚¬í•­
        security_score = self._calculate_security_score(vulnerabilities)
        immediate_actions = self._generate_immediate_actions(vulnerabilities)
        best_practices = self._generate_best_practices(vulnerabilities)
        
        return {
            "success": True,
            "analysis": {
                "code_vulnerabilities": vulnerabilities,
                "security_score": security_score,
                "summary": self._generate_summary(vulnerabilities),
                "immediate_actions": immediate_actions,
                "best_practices": best_practices
            },
            "metadata": {
                "gpt_model": self.model,
                "rag_available": self.rag_available,
                "total_vulnerabilities": len(vulnerabilities)
            }
        }
    
    def _gpt_analyze(self, code: str) -> Dict:
        """GPTë¡œ ì·¨ì•½ì  ë¶„ì„"""
        
        # ì½”ë“œì— ë¼ì¸ ë²ˆí˜¸ ì¶”ê°€
        lines = code.split('\n')
        code_with_lines = '\n'.join([f"{i+1:3}: {line}" for i, line in enumerate(lines)])
        
        if PROMPTS_AVAILABLE:
            prompt = get_analysis_prompt(code_with_lines)
        else:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            prompt = f"""
            Analyze this Python code for security vulnerabilities.
            
            Code:
            ```python
            {code_with_lines}
            ```
            
            Important:
            - Parameter binding (?, %s with tuple) is SAFE from SQL injection
            - Environment variables are SAFE for secrets
            - bcrypt, argon2, pbkdf2 are SAFE for password hashing
            
            Return JSON:
            {{
                "vulnerabilities": [
                    {{
                        "type": "vulnerability type",
                        "severity": "CRITICAL/HIGH/MEDIUM/LOW",
                        "line_numbers": [line numbers],
                        "vulnerable_code": "code snippet",
                        "description": "why it's vulnerable",
                        "recommendation": "how to fix"
                    }}
                ]
            }}
            """
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=2500
            )
            
            result_text = response.choices[0].message.content
            
            # JSON íŒŒì‹±
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0]
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0]
            
            result = json.loads(result_text.strip())
            
            # ì˜¤íƒ í•„í„°ë§ (íŒŒë¼ë¯¸í„° ë°”ì¸ë”©ì„ SQL injectionìœ¼ë¡œ ì˜ëª» íŒë‹¨í•œ ê²½ìš°)
            filtered_vulns = []
            for vuln in result.get('vulnerabilities', []):
                # SQL Injection + íŒŒë¼ë¯¸í„° ë°”ì¸ë”© ì²´í¬
                if vuln.get('type') == 'SQL Injection' and vuln.get('vulnerable_code'):
                    if '?' in vuln['vulnerable_code'] or \
                       ('execute' in vuln['vulnerable_code'] and ',' in vuln['vulnerable_code']):
                        # íŒŒë¼ë¯¸í„° ë°”ì¸ë”©ì€ ì•ˆì „í•¨ - ìŠ¤í‚µ
                        print(f"  âœ… False positive ì œê±°: íŒŒë¼ë¯¸í„° ë°”ì¸ë”©ì€ ì•ˆì „í•©ë‹ˆë‹¤")
                        continue
                
                filtered_vulns.append(vuln)
            
            result['vulnerabilities'] = filtered_vulns
            return result
            
        except Exception as e:
            print(f"âŒ GPT ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"vulnerabilities": []}
    
    def _enhance_with_rag(self, vulnerabilities: List[Dict]):
        """RAGë¡œ ì„¤ëª… ë³´ê°•"""
        if not self.rag_available or not self.rag:
            return
        
        # ê°„ë‹¨íˆ ì²˜ë¦¬
        for vuln in vulnerabilities[:5]:  # ìµœëŒ€ 5ê°œë§Œ
            vuln_type = vuln.get('type', '')
            
            try:
                # RAG ê²€ìƒ‰
                results = self.rag.search_similar(vuln_type, top_k=1)
                if results['documents'][0]:
                    doc = results['documents'][0][0][:200]
                    vuln['rag_context'] = doc
                    vuln['explanation_source'] = 'AI + KISIA ê°€ì´ë“œë¼ì¸'
            except:
                pass
    
    def _calculate_security_score(self, vulnerabilities: List[Dict]) -> int:
        """ë³´ì•ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 100
        
        for vuln in vulnerabilities:
            severity = vuln.get('severity', 'MEDIUM')
            
            penalty = {
                'CRITICAL': 25,
                'HIGH': 15,
                'MEDIUM': 10,
                'LOW': 5
            }.get(severity, 10)
            
            score -= penalty
        
        return max(0, score)
    
    def _generate_immediate_actions(self, vulnerabilities: List[Dict]) -> List[str]:
        """ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­ ìƒì„±"""
        actions = []
        
        critical_vulns = [v for v in vulnerabilities if v.get('severity') in ['CRITICAL', 'HIGH']]
        
        for vuln in critical_vulns[:5]:
            line = vuln.get('line_numbers', [0])[0]
            actions.append(f"ë¼ì¸ {line}: {vuln['type']} ì¦‰ì‹œ ìˆ˜ì • í•„ìš”")
        
        if not actions:
            actions.append("ì‹¬ê°í•œ ì·¨ì•½ì ì€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìœ¼ë‚˜, ì „ì²´ ì·¨ì•½ì ì„ ê²€í† í•˜ì„¸ìš”.")
        
        return actions
    
    def _generate_best_practices(self, vulnerabilities: List[Dict]) -> List[str]:
        """ë³´ì•ˆ ëª¨ë²” ì‚¬ë¡€ ìƒì„±"""
        practices = set()
        
        for vuln in vulnerabilities:
            vuln_type = vuln.get('type', '')
            
            if 'Injection' in vuln_type:
                practices.add("ëª¨ë“  ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•´ íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©")
            elif 'Cryptography' in vuln_type:
                practices.add("ê°•ë ¥í•œ ì•”í˜¸í™” ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© (AES-256, SHA-256 ì´ìƒ)")
            elif 'Secret' in vuln_type or 'Password' in vuln_type:
                practices.add("ë¯¼ê°í•œ ì •ë³´ëŠ” í™˜ê²½ ë³€ìˆ˜ë‚˜ ë³´ì•ˆ ì €ì¥ì†Œì— ë³´ê´€")
            elif 'Validation' in vuln_type:
                practices.add("ëª¨ë“  ì…ë ¥ê°’ì— ëŒ€í•œ ê²€ì¦ ë° sanitization ìˆ˜í–‰")
        
        if not practices:
            practices.add("ì •ê¸°ì ì¸ ë³´ì•ˆ ê°ì‚¬ ì‹¤ì‹œ")
            practices.add("ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì •ê¸° ì—…ë°ì´íŠ¸")
        
        return list(practices)
    
    def _generate_summary(self, vulnerabilities: List[Dict]) -> str:
        """ë¶„ì„ ìš”ì•½ ìƒì„±"""
        if not vulnerabilities:
            return "ë³´ì•ˆ ì·¨ì•½ì ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        total = len(vulnerabilities)
        critical = len([v for v in vulnerabilities if v.get('severity') == 'CRITICAL'])
        high = len([v for v in vulnerabilities if v.get('severity') == 'HIGH'])
        
        summary = f"ì´ {total}ê°œì˜ ë³´ì•ˆ ì·¨ì•½ì  ë°œê²¬"
        if critical > 0:
            summary += f" (CRITICAL: {critical}ê°œ)"
        if high > 0:
            summary += f" (HIGH: {high}ê°œ)"
        
        return summary


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
ImprovedLLMAnalyzer = LLMSecurityAnalyzer


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    test_code = """
import sqlite3

def safe_query(user_id):
    conn = sqlite3.connect('db.db')
    cursor = conn.cursor()
    cursor.execute('SELECT * FROM users WHERE id = ?', (user_id,))
    return cursor.fetchall()

def unsafe_query(name):
    conn = sqlite3.connect('db.db')
    cursor = conn.cursor()
    query = f"SELECT * FROM users WHERE name = '{name}'"
    cursor.execute(query)
    return cursor.fetchall()
"""
    
    try:
        analyzer = LLMSecurityAnalyzer()
        result = analyzer.analyze_code_security(test_code)
        print(json.dumps(result, indent=2, ensure_ascii=False))
    except Exception as e:
        print(f"Error: {e}")