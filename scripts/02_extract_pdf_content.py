# scripts/02_extract_pdf_content.py
"""
PDFì—ì„œ í…ìŠ¤íŠ¸ì™€ ì½”ë“œë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ì¶”ì¶œ
ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì¶”ì¶œ
"""
import pdfplumber
import json
from pathlib import Path
from typing import List, Dict, Tuple
import re
from dataclasses import dataclass, asdict

@dataclass
class CodeExample:
    """ì½”ë“œ ì˜ˆì œ êµ¬ì¡°ì²´"""
    page: int
    type: str  # 'unsafe' or 'safe'
    code: str
    context: str  # ì½”ë“œ ì•ë’¤ ì„¤ëª…
    vulnerability_type: List[str]
    label: str

@dataclass
class VulnerabilitySection:
    """ì·¨ì•½ì  ì„¹ì…˜ êµ¬ì¡°ì²´"""
    title: str
    description: str
    unsafe_code: CodeExample
    safe_code: CodeExample
    page_range: Tuple[int, int]
    recommendations: str

class PDFContentExtractor:
    def __init__(self, pdf_path: str, analysis_path: str = "data/processed/metadata/pdf_analysis_fixed.json"):
        self.pdf_path = Path(pdf_path)
        
        # ì´ì „ ë¶„ì„ ê²°ê³¼ ë¡œë“œ
        with open(analysis_path, 'r', encoding='utf-8') as f:
            self.analysis = json.load(f)
        
        self.content = {
            "vulnerability_sections": [],
            "code_examples": [],
            "chunks": [],
            "metadata": {
                "total_pages": self.analysis['total_pages'],
                "total_sections": 0,
                "total_chunks": 0
            }
        }
        
    def extract(self):
        """PDF ë‚´ìš© ì¶”ì¶œ"""
        print(f"ğŸ“„ PDF ë‚´ìš© ì¶”ì¶œ ì‹œì‘: {self.pdf_path.name}")
        
        with pdfplumber.open(self.pdf_path) as pdf:
            # 1. ì½”ë“œ ìŒ ê¸°ë°˜ìœ¼ë¡œ ì·¨ì•½ì  ì„¹ì…˜ ì¶”ì¶œ
            self._extract_vulnerability_sections(pdf)
            
            # 2. ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹
            self._create_semantic_chunks(pdf)
            
            # 3. ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self._update_metadata()
        
        print(f"âœ… ì¶”ì¶œ ì™„ë£Œ")
        return self.content
    
    def _extract_vulnerability_sections(self, pdf):
        """ì·¨ì•½ì  ì„¹ì…˜ ì¶”ì¶œ"""
        print(f"ğŸ” ì·¨ì•½ì  ì„¹ì…˜ ì¶”ì¶œ ì¤‘...")
        
        code_pairs = self.analysis.get('code_pairs', [])
        
        for pair in code_pairs:
            unsafe_code = pair['unsafe']
            safe_code = pair['safe']
            
            # í˜ì´ì§€ ë²”ìœ„ ê²°ì •
            start_page = min(unsafe_code['page'], safe_code['page'])
            end_page = max(unsafe_code['page'], safe_code['page'])
            
            # í•´ë‹¹ í˜ì´ì§€ë“¤ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            section_text = ""
            for page_num in range(start_page - 1, min(end_page + 1, len(pdf.pages))):
                page = pdf.pages[page_num]
                section_text += page.extract_text() + "\n"
            
            # ì„¹ì…˜ ì œëª© ì°¾ê¸°
            title = self._find_section_title(section_text, start_page)
            
            # ì„¤ëª… í…ìŠ¤íŠ¸ ì¶”ì¶œ
            description = self._extract_description(section_text, unsafe_code, safe_code)
            
            # ê¶Œì¥ì‚¬í•­ ì¶”ì¶œ
            recommendations = self._extract_recommendations(section_text)
            
            # VulnerabilitySection ìƒì„±
            vuln_section = {
                "title": title,
                "description": description,
                "unsafe_code": {
                    "page": unsafe_code['page'],
                    "code": unsafe_code.get('code', ''),
                    "label": unsafe_code.get('label', 'ì•ˆì „í•˜ì§€ ì•Šì€ ì½”ë“œ ì˜ˆì‹œ')
                },
                "safe_code": {
                    "page": safe_code['page'],
                    "code": safe_code.get('code', ''),
                    "label": safe_code.get('label', 'ì•ˆì „í•œ ì½”ë“œ ì˜ˆì‹œ')
                },
                "page_range": [start_page, end_page],
                "recommendations": recommendations,
                "vulnerability_types": pair.get('vulnerability_type', ['General'])
            }
            
            self.content["vulnerability_sections"].append(vuln_section)
        
        print(f"  âœ“ {len(self.content['vulnerability_sections'])}ê°œ ì·¨ì•½ì  ì„¹ì…˜ ì¶”ì¶œ")
    
    def _create_semantic_chunks(self, pdf):
        """ì˜ë¯¸ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ì²­í‚¹"""
        print(f"ğŸ“ ì˜ë¯¸ ë‹¨ìœ„ ì²­í‚¹ ì¤‘...")
        
        for page_num, page in enumerate(pdf.pages, 1):
            if page_num % 30 == 0:
                print(f"  ì²˜ë¦¬ ì¤‘... {page_num}/{len(pdf.pages)} í˜ì´ì§€")
            
            text = page.extract_text()
            if not text:
                continue
            
            # í˜ì´ì§€ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• 
            chunks = self._split_into_chunks(text, page_num)
            
            for chunk in chunks:
                # ì²­í¬ ë¶„ë¥˜
                chunk_type = self._classify_chunk(chunk['text'])
                
                self.content["chunks"].append({
                    "page": page_num,
                    "text": chunk['text'],
                    "type": chunk_type,
                    "metadata": {
                        "char_count": len(chunk['text']),
                        "has_code": self._has_code(chunk['text']),
                        "keywords": self._extract_keywords(chunk['text'])
                    }
                })
        
        print(f"  âœ“ {len(self.content['chunks'])}ê°œ ì²­í¬ ìƒì„±")
    
    def _split_into_chunks(self, text: str, page_num: int, 
                          chunk_size: int = 800, 
                          overlap: int = 200) -> List[Dict]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        
        # ë‹¨ë½ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„í• 
        paragraphs = text.split('\n\n')
        
        current_chunk = ""
        for para in paragraphs:
            # ì²­í¬ í¬ê¸° í™•ì¸
            if len(current_chunk) + len(para) < chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append({
                        'text': current_chunk.strip(),
                        'page': page_num
                    })
                
                # ì˜¤ë²„ë© ì²˜ë¦¬
                if len(current_chunk) > overlap:
                    overlap_text = current_chunk[-overlap:]
                    current_chunk = overlap_text + para + "\n\n"
                else:
                    current_chunk = para + "\n\n"
        
        # ë§ˆì§€ë§‰ ì²­í¬
        if current_chunk.strip():
            chunks.append({
                'text': current_chunk.strip(),
                'page': page_num
            })
        
        return chunks
    
    def _classify_chunk(self, text: str) -> str:
        """ì²­í¬ ë¶„ë¥˜"""
        text_lower = text.lower()
        
        # ì½”ë“œ ì²­í¬
        if any(pattern in text for pattern in ['def ', 'class ', 'import ', 'if __name__']):
            return 'code'
        
        # ì·¨ì•½ì  ì„¤ëª…
        if any(word in text_lower for word in ['ì·¨ì•½ì ', 'ê³µê²©', 'injection', 'xss', 'csrf']):
            return 'vulnerability'
        
        # ê¶Œì¥ì‚¬í•­
        if any(word in text_lower for word in ['ê¶Œì¥', 'í•´ì•¼', 'ì£¼ì˜', 'ë°©ì§€', 'ë³´ì•ˆ']):
            return 'recommendation'
        
        # ì¼ë°˜ ì„¤ëª…
        return 'general'
    
    def _has_code(self, text: str) -> bool:
        """ì½”ë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
        code_patterns = [
            r'def \w+\(',
            r'class \w+',
            r'import \w+',
            r'\w+\.\w+\(',
            r'if .+:',
            r'for .+ in .+:',
        ]
        
        for pattern in code_patterns:
            if re.search(pattern, text):
                return True
        return False
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ë³´ì•ˆ ê´€ë ¨ í‚¤ì›Œë“œ
        security_keywords = [
            'SQL', 'XSS', 'CSRF', 'injection', 'ì·¨ì•½ì ', 'ë³´ì•ˆ',
            'ì•”í˜¸í™”', 'ì¸ì¦', 'ì¸ê°€', 'ì„¸ì…˜', 'ì¿ í‚¤', 'token',
            'escape', 'sanitize', 'validate', 'filter'
        ]
        
        for keyword in security_keywords:
            if keyword.lower() in text.lower():
                keywords.append(keyword)
        
        return keywords[:5]  # ìµœëŒ€ 5ê°œ
    
    def _find_section_title(self, text: str, page_num: int) -> str:
        """ì„¹ì…˜ ì œëª© ì°¾ê¸°"""
        lines = text.split('\n')
        
        # ì œëª© íŒ¨í„´
        title_patterns = [
            r'^(\d+\.[\d\.]*)\s+(.+)$',  # 1.2.3 ì œëª©
            r'^(ì œ\d+[ì¥ì ˆ])\s+(.+)$',    # ì œ1ì¥ ì œëª©
            r'^\[(.+)\]$',                # [ì œëª©]
        ]
        
        for line in lines[:20]:  # ì²˜ìŒ 20ì¤„ë§Œ í™•ì¸
            for pattern in title_patterns:
                match = re.match(pattern, line.strip())
                if match:
                    return line.strip()
        
        return f"Section (Page {page_num})"
    
    def _extract_description(self, text: str, unsafe_code: Dict, safe_code: Dict) -> str:
        """ì„¤ëª… ì¶”ì¶œ"""
        # ì½”ë“œ ì „í›„ì˜ í…ìŠ¤íŠ¸ë¥¼ ì„¤ëª…ìœ¼ë¡œ ì¶”ì¶œ
        lines = text.split('\n')
        
        # "ì•ˆì „í•˜ì§€ ì•Šì€ ì½”ë“œ ì˜ˆì‹œ" ì´ì „ í…ìŠ¤íŠ¸ ì°¾ê¸°
        description_lines = []
        for i, line in enumerate(lines):
            if 'ì•ˆì „í•˜ì§€ ì•Šì€ ì½”ë“œ' in line:
                # ì´ì „ 10ì¤„ ì •ë„ë¥¼ ì„¤ëª…ìœ¼ë¡œ
                start = max(0, i - 10)
                description_lines = lines[start:i]
                break
        
        description = '\n'.join(description_lines)
        
        # ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½
        if len(description) > 1000:
            description = description[:1000] + "..."
        
        return description.strip()
    
    def _extract_recommendations(self, text: str) -> str:
        """ê¶Œì¥ì‚¬í•­ ì¶”ì¶œ"""
        recommendations = []
        
        # ê¶Œì¥ì‚¬í•­ í‚¤ì›Œë“œ
        rec_keywords = ['ê¶Œì¥', 'í•´ì•¼', 'ì‚¬ìš©í•˜ì„¸ìš”', 'ì£¼ì˜', 'ë°©ì§€', 'í™•ì¸']
        
        lines = text.split('\n')
        for line in lines:
            if any(keyword in line for keyword in rec_keywords):
                recommendations.append(line.strip())
        
        # ìµœëŒ€ 5ê°œ ê¶Œì¥ì‚¬í•­
        return '\n'.join(recommendations[:5])
    
    def _update_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        self.content["metadata"]["total_sections"] = len(self.content["vulnerability_sections"])
        self.content["metadata"]["total_chunks"] = len(self.content["chunks"])
        
        # ì²­í¬ íƒ€ì…ë³„ í†µê³„
        chunk_types = {}
        for chunk in self.content["chunks"]:
            chunk_type = chunk['type']
            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
        
        self.content["metadata"]["chunk_types"] = chunk_types
    
    def save_results(self, output_dir: str = "data/processed"):
        """ì¶”ì¶œ ê²°ê³¼ ì €ì¥"""
        output_dir = Path(output_dir)
        
        # ì·¨ì•½ì  ì„¹ì…˜ ì €ì¥
        vuln_path = output_dir / "chunks" / "vulnerability_sections.json"
        vuln_path.parent.mkdir(parents=True, exist_ok=True)
        with open(vuln_path, 'w', encoding='utf-8') as f:
            json.dump(self.content["vulnerability_sections"], f, 
                     ensure_ascii=False, indent=2)
        print(f"âœ… ì·¨ì•½ì  ì„¹ì…˜ ì €ì¥: {vuln_path}")
        
        # ì²­í¬ ì €ì¥
        chunks_path = output_dir / "chunks" / "semantic_chunks.json"
        with open(chunks_path, 'w', encoding='utf-8') as f:
            json.dump(self.content["chunks"], f, 
                     ensure_ascii=False, indent=2)
        print(f"âœ… ì²­í¬ ì €ì¥: {chunks_path}")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        meta_path = output_dir / "metadata" / "extraction_metadata.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(self.content["metadata"], f, 
                     ensure_ascii=False, indent=2)
        print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {meta_path}")
    
    def print_summary(self):
        """ì¶”ì¶œ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š PDF ë‚´ìš© ì¶”ì¶œ ê²°ê³¼")
        print("="*60)
        
        print(f"\nğŸ“„ íŒŒì¼: {self.pdf_path.name}")
        print(f"ğŸ“‘ ì´ í˜ì´ì§€: {self.content['metadata']['total_pages']}")
        
        print(f"\nğŸ¯ ì·¨ì•½ì  ì„¹ì…˜:")
        print(f"  â€¢ ì´ ì„¹ì…˜: {self.content['metadata']['total_sections']}ê°œ")
        
        # ì·¨ì•½ì  íƒ€ì…ë³„ í†µê³„
        vuln_types = {}
        for section in self.content["vulnerability_sections"]:
            for vtype in section.get('vulnerability_types', ['General']):
                vuln_types[vtype] = vuln_types.get(vtype, 0) + 1
        
        print(f"\n  ì·¨ì•½ì  íƒ€ì…ë³„:")
        for vtype, count in sorted(vuln_types.items(), key=lambda x: x[1], reverse=True):
            print(f"    â€¢ {vtype}: {count}ê°œ")
        
        print(f"\nğŸ“ ì²­í¬ ë¶„ì„:")
        print(f"  â€¢ ì´ ì²­í¬: {self.content['metadata']['total_chunks']}ê°œ")
        
        if 'chunk_types' in self.content['metadata']:
            print(f"\n  ì²­í¬ íƒ€ì…ë³„:")
            for ctype, count in self.content['metadata']['chunk_types'].items():
                print(f"    â€¢ {ctype}: {count}ê°œ")

if __name__ == "__main__":
    # PDF ê²½ë¡œ
    pdf_path = "data/guidelines/Python_ì‹œíì–´ì½”ë”©_ê°€ì´ë“œ(2023ë…„_ê°œì •ë³¸).pdf"
    
    # ì¶”ì¶œê¸° ìƒì„± ë° ì‹¤í–‰
    extractor = PDFContentExtractor(pdf_path)
    content = extractor.extract()
    
    # ê²°ê³¼ ì €ì¥
    extractor.save_results()
    
    # ìš”ì•½ ì¶œë ¥
    extractor.print_summary()