# scripts/prepare_vectors_v2.py
"""
PDF ì „ì²´ë¥¼ ë²¡í„°í™”í•˜ëŠ” ê°œì„ ëœ ìŠ¤í¬ë¦½íŠ¸
ChromaDB ë©”íƒ€ë°ì´í„° í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
"""
import pdfplumber
import chromadb
from chromadb.config import Settings
import re
from pathlib import Path
from typing import List, Dict
import hashlib

class PDFProcessor:
    def __init__(self, pdf_path: str):
        self.pdf_path = pdf_path
        self.chroma_client = chromadb.PersistentClient(
            path="data/vector_db",
            settings=Settings(anonymized_telemetry=False)
        )
    
    def process(self):
        """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜"""
        print("ğŸ“š PDF ì²˜ë¦¬ ì‹œì‘")
        
        # PDF ì •ë³´
        with pdfplumber.open(self.pdf_path) as pdf:
            total_pages = len(pdf.pages)
            print(f"ğŸ“– ì „ì²´: {total_pages} í˜ì´ì§€")
        
        # ë²”ìœ„ ì„¤ì •
        start = 8
        end = min(171, total_pages)  # ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        print(f"ğŸ“„ ì²˜ë¦¬ ë²”ìœ„: {start}-{end} í˜ì´ì§€")
        
        # ì¶”ì¶œ
        chunks = self.extract_chunks(start, end)
        
        # DB ìƒì„±
        self.create_db(chunks)
        
        print(f"\nâœ… ì™„ë£Œ! {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    def extract_chunks(self, start: int, end: int) -> List[Dict]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        
        with pdfplumber.open(self.pdf_path) as pdf:
            current_text = ""
            current_page = start
            
            for page_num in range(start-1, min(end, len(pdf.pages))):
                page = pdf.pages[page_num]
                text = page.extract_text()
                
                if not text:
                    continue
                
                current_text += text + "\n"
                
                # 2000ìë§ˆë‹¤ ì²­í¬ ìƒì„±
                while len(current_text) > 2000:
                    chunk_text = current_text[:2000]
                    chunks.append({
                        'text': chunk_text,
                        'page': page_num + 1
                    })
                    current_text = current_text[1800:]  # ì•½ê°„ ê²¹ì¹˜ê²Œ
                
                if (page_num + 1) % 10 == 0:
                    print(f"  âœ“ {page_num + 1}/{end} í˜ì´ì§€ ì²˜ë¦¬")
            
            # ë‚¨ì€ í…ìŠ¤íŠ¸
            if current_text.strip():
                chunks.append({
                    'text': current_text,
                    'page': end
                })
        
        print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        return chunks
    
    def create_db(self, chunks: List[Dict]):
        """ë²¡í„° DB ìƒì„±"""
        print("\nğŸ—„ï¸ ë²¡í„° DB ìƒì„± ì¤‘...")
        
        # ê¸°ì¡´ ì‚­ì œ
        try:
            self.chroma_client.delete_collection("secure_coding_guide")
            print("  âœ“ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ")
        except:
            pass
        
        # ìƒˆ ì»¬ë ‰ì…˜
        collection = self.chroma_client.create_collection(
            name="secure_coding_guide",
            metadata={"description": "Python ì‹œíì–´ì½”ë”© ê°€ì´ë“œ"}
        )
        
        # ë°ì´í„° ì¤€ë¹„
        batch_size = 40
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]
            
            documents = []
            metadatas = []
            ids = []
            
            for j, chunk in enumerate(batch):
                # í…ìŠ¤íŠ¸
                text = chunk['text']
                documents.append(text)
                
                # ë©”íƒ€ë°ì´í„° - ë‹¨ìˆœí•˜ê²Œ ìœ ì§€
                metadata = {
                    'page': chunk['page'],
                    'chunk_index': i + j,
                    'length': len(text)
                }
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ (ë¬¸ìì—´ë¡œ)
                keywords = self.extract_keywords(text)
                if keywords:
                    metadata['keywords'] = keywords  # ì´ë¯¸ ë¬¸ìì—´
                
                metadatas.append(metadata)
                
                # ID
                ids.append(f"chunk_{i+j:04d}")
            
            # ì €ì¥
            try:
                collection.add(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                print(f"  âœ“ {min(i+batch_size, len(chunks))}/{len(chunks)} ì €ì¥")
            except Exception as e:
                print(f"  âŒ ë°°ì¹˜ {i//batch_size + 1} ì‹¤íŒ¨: {e}")
                # ë¬¸ì œ ìˆëŠ” ë©”íƒ€ë°ì´í„° í™•ì¸
                for m in metadatas:
                    for k, v in m.items():
                        if not isinstance(v, (str, int, float, bool, type(None))):
                            print(f"    ë¬¸ì œ: {k} = {v} (íƒ€ì…: {type(v)})")
        
        print("âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸
        self.test_search(collection)
    
    def extract_keywords(self, text: str) -> str:
        """í‚¤ì›Œë“œ ì¶”ì¶œ - ë¬¸ìì—´ë¡œ ë°˜í™˜"""
        keywords = []
        
        # ë³´ì•ˆ í‚¤ì›Œë“œ
        patterns = [
            'SQL.?ì‚½ì…', 'SQL.?ì¸ì ì…˜', 'XSS', 'í¬ë¡œìŠ¤ì‚¬ì´íŠ¸',
            'CSRF', 'ì¸ì¦', 'ì¸ê°€', 'ì•”í˜¸í™”', 'í•´ì‹œ', 'íŒ¨ìŠ¤ì›Œë“œ',
            'íŒŒì¼.?ì—…ë¡œë“œ', 'ê²½ë¡œ.?ì¡°ì‘', 'LDAP', 'ì½”ë“œ.?ì‚½ì…'
        ]
        
        for pattern in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                keywords.append(pattern.replace('.?', ''))
        
        # ìµœëŒ€ 5ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´
        return ', '.join(keywords[:5])
    
    def test_search(self, collection):
        """í…ŒìŠ¤íŠ¸ ê²€ìƒ‰"""
        print("\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰:")
        
        queries = [
            "SQL ì¸ì ì…˜",
            "íŒ¨ìŠ¤ì›Œë“œ ì €ì¥",
            "XSS ë°©ì§€",
            "íŒŒì¼ ì—…ë¡œë“œ"
        ]
        
        for query in queries:
            results = collection.query(
                query_texts=[query],
                n_results=1
            )
            
            if results['documents'][0]:
                doc = results['documents'][0][0][:100] + "..."
                meta = results['metadatas'][0][0]
                print(f"  âœ“ '{query}' â†’ í˜ì´ì§€ {meta.get('page', '?')}")

def main():
    pdf_path = "data/guidelines/Python_ì‹œíì–´ì½”ë”©_ê°€ì´ë“œ(2023ë…„_ê°œì •ë³¸).pdf"
    
    if not Path(pdf_path).exists():
        print(f"âŒ PDF ì—†ìŒ: {pdf_path}")
        return
    
    processor = PDFProcessor(pdf_path)
    processor.process()

if __name__ == "__main__":
    main()